{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Search - Index (Pull) documents for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docs\n",
    "\n",
    "- https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search\n",
    "\n",
    "Basic appoaches push and pull\n",
    "- https://learn.microsoft.com/en-us/azure/search/search-what-is-data-import\n",
    "- Note: If AI enrichment (https://learn.microsoft.com/en-us/azure/search/cognitive-search-concept-intro) is a solution requirement, you must use the pull model (indexers) to load an index. Skillsets are attached to an indexer and don't run independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspirational sources\n",
    "- https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/integrated-vectorization/azure-search-integrated-vectorization-sample.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "- https://learn.microsoft.com/en-us/azure/search/search-api-versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gobal flags (e.g. for debug and development)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .env file (Copy .env-sample to .env and update accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True) # take environment variables from .env.\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = AzureKeyCredential(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) if len(os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]) > 0 else DefaultAzureCredential()\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "\n",
    "blob_connection_string = os.environ[\"BLOB_CONNECTION_STRING\"]\n",
    "blob_container_name = os.environ[\"BLOB_CONTAINER_NAME\"]\n",
    "\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "azure_openai_embedding_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Blob Storage and sync documents\n",
    "Synchronize documents in the blob storage with local document data. This will delete any documents that are not present locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup sample data in openai...\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient  \n",
    "import os\n",
    "\n",
    "# Connect to Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "container_client = blob_service_client.get_container_client(blob_container_name)\n",
    "if not container_client.exists():\n",
    "    container_client.create_container()\n",
    "\n",
    "deprecated_blobs = [blob for blob in container_client.list_blob_names()]\n",
    "\n",
    "print(f\"Setup sample data in {blob_container_name}...\")\n",
    "\n",
    "documents_directory = os.path.join(\"data\")\n",
    "for file in os.listdir(documents_directory):\n",
    "    with open(os.path.join(documents_directory, file), \"rb\") as data:\n",
    "        name = os.path.basename(file)\n",
    "        if not container_client.get_blob_client(name).exists():\n",
    "            print(f'Uploading: {name}')\n",
    "            container_client.upload_blob(name=name, data=data)\n",
    "\n",
    "        if name in deprecated_blobs:\n",
    "                deprecated_blobs.remove(name)\n",
    "\n",
    "if len(deprecated_blobs) > 0:\n",
    "    for _blob_name in deprecated_blobs:\n",
    "        print(f\"Deleting (not found locally): {_blob_name}\")\n",
    "        container_client.delete_blob(_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a blob data source connection on Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'testindex150424v4-blob' created or updated\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import SearchIndexerDataContainer, SearchIndexerDataSourceConnection\n",
    "\n",
    "# Create a data source \n",
    "indexer_client = SearchIndexerClient(endpoint, credential)\n",
    "container = SearchIndexerDataContainer(name=blob_container_name)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=blob_connection_string,\n",
    "    container=container\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a search index\n",
    "Vector and nonvector content is stored in a search index. Note the key attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testindex150424v3 created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIParameters,\n",
    "    SemanticConfiguration,\n",
    "    SemanticSearch,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "# Create a search index  \n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)  \n",
    "fields = [  \n",
    "    SearchField(name=\"id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String, sortable=False, filterable=True, facetable=False, searchable=False),  \n",
    "    # SearchField(name=\"type\", type=SearchFieldDataType.Int32, sortable=False, filterable=True, facetable=False, searchable=False),  \n",
    "    SearchField(name=\"url\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False, searchable=False),  \n",
    "    SearchField(name=\"filepath\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False, searchable=False),  \n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False, key=True, analyzer_name=\"keyword\"),  \n",
    "    SearchField(name=\"content\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False, searchable=True), \n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),  \n",
    "    SearchField(name=\"last_updated\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False, searchable=False)\n",
    "]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        # Note:\n",
    "        # HHNSW has several configuration parameters that can be tuned to achieve the throughput, latency, and recall objectives for your search application.\n",
    "        # https://learn.microsoft.com/en-us/azure/search/vector-search-ranking#when-to-use-hnsw\n",
    "        HnswAlgorithmConfiguration(  \n",
    "            name=\"myHnsw\",  \n",
    "            parameters=HnswParameters(  \n",
    "                m=4,  \n",
    "                ef_construction=400,  \n",
    "                ef_search=500,  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),  \n",
    "        ),  \n",
    "        # Note: ExhaustiveKnn is not actually used in the defintition of the index fields.\n",
    "        # Exhaustive KNN performs a brute-force search that scans the entire vector space.\n",
    "        # It's intended for scenarios where high recall is of utmost importance, and users are willing to accept the trade-offs in search performance. Because it's computationally intensive, use exhaustive KNN for small to medium datasets, or when precision requirements outweigh query performance considerations.\n",
    "        # https://learn.microsoft.com/en-us/azure/search/vector-search-ranking#when-to-use-exhaustive-knn\n",
    "        ExhaustiveKnnAlgorithmConfiguration(  \n",
    "            name=\"myExhaustiveKnn\",  \n",
    "            parameters=ExhaustiveKnnParameters(  \n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myExhaustiveKnnProfile\",  \n",
    "            algorithm_configuration_name=\"myExhaustiveKnn\",  \n",
    "            vectorizer=\"myOpenAI\",  \n",
    "        ),  \n",
    "    ],  \n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(  \n",
    "                resource_uri=azure_openai_endpoint,  \n",
    "                deployment_id=azure_openai_embedding_deployment,  \n",
    "                api_key=azure_openai_key,  \n",
    "            ),  \n",
    "        ),  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "# https://learn.microsoft.com/en-us/python/api/azure-search-documents/azure.search.documents.indexes.models.semanticconfiguration?view=azure-python-preview\n",
    "semantic_config = SemanticConfiguration(  \n",
    "    name=\"my-semantic-config\",  \n",
    "    prioritized_fields=SemanticPrioritizedFields(  \n",
    "        content_fields=[SemanticField(field_name=\"content\")]  \n",
    "    ),  \n",
    ")  \n",
    "  \n",
    "# Create the semantic search with the configuration  \n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])  \n",
    "\n",
    "# Delete the current index (for testing purposes only)\n",
    "index_client.delete_index(index_name)  \n",
    "\n",
    "# Create the search index\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search, semantic_search=semantic_search)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custome skill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import WebApiSkill, InputFieldMappingEntry, OutputFieldMappingEntry\n",
    "\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-search-documents/azure.search.documents.indexes.models.webapiskill?view=azure-python\n",
    "custom_skill = WebApiSkill(\n",
    "    name=\"mindscape-skill\",\n",
    "    description=\"A custom skill\",\n",
    "    context=\"/document\",\n",
    "    uri=\"https://sbdnic-func-prod-weu-aisearch-skill.azurewebsites.net/api/prepare\",\n",
    "    http_method=\"POST\",\n",
    "    timeout=\"PT30S\",\n",
    "    batch_size=1,\n",
    "    degree_of_parallelism=1,\n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"type\", target_name=\"type\")  \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a skillset\n",
    "Skills drive integrated vectorization. Text Split provides data chunking. AzureOpenAIEmbedding handles calls to Azure OpenAI, using the connection information you provide in the environment variables. An indexer projection specifies secondary indexes used for chunked data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testindex150424v4-skillset created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    SearchIndexerIndexProjections,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    SearchIndexerSkillset\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = f\"{index_name}-skillset\"  \n",
    "  \n",
    "\n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=2000,  \n",
    "    page_overlap_length=500,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_uri=azure_openai_endpoint,  \n",
    "    deployment_id=azure_openai_embedding_deployment,  \n",
    "    api_key=azure_openai_key,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        # OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\") \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"contentVector\")  \n",
    "    ],  \n",
    ")  \n",
    "  \n",
    "# For standard meta data see: \n",
    "# https://learn.microsoft.com/en-us/azure/search/search-howto-indexing-azure-blob-storage \n",
    "# https://learn.microsoft.com/en-us/azure/search/search-blob-metadata-properties\n",
    "index_projections = SearchIndexerIndexProjections(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=index_name,  \n",
    "            # parent_key_field_name=\"parent_id\",  \n",
    "            parent_key_field_name=\"id\",  \n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"content\", source=\"/document/pages/*\"),  \n",
    "                InputFieldMappingEntry(name=\"contentVector\", source=\"/document/pages/*/contentVector\"),  \n",
    "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),  \n",
    "                # InputFieldMappingEntry(name=\"content\", source=\"/document/content\"),  \n",
    "                # InputFieldMappingEntry(name=\"type\", source=\"/document/type\"),  \n",
    "                InputFieldMappingEntry(name=\"filepath\", source=\"/document/metadata_storage_path\"),  \n",
    "                InputFieldMappingEntry(name=\"url\", source=\"/document/metadata_storage_path\"),  \n",
    "                InputFieldMappingEntry(name=\"last_updated\", source=\"/document/metadata_storage_last_modified\"),  \n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ")  \n",
    "  \n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    # skills=[custom_skill, split_skill, embedding_skill],  \n",
    "    skills=[split_skill, embedding_skill],  \n",
    "    index_projections=index_projections,  \n",
    ")  \n",
    "  \n",
    "client = SearchIndexerClient(endpoint, credential)  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f\"{skillset.name} created\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " testindex150424v4-indexer created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndexer #, FieldMapping\n",
    "\n",
    "# Create an indexer  \n",
    "indexer_name = f\"{index_name}-indexer\"  \n",
    "  \n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,  \n",
    "    # Map the metadata_storage_name field to the title field in the index to display the PDF title in the search results  \n",
    "    # RH: this is done further below more explicitly:\n",
    "    # field_mappings=[FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\")]  \n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(endpoint, credential)  \n",
    "\n",
    "# delete indexer (for testing purposes only)\n",
    "indexer_client.delete_indexer(indexer_name)\n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "print(f' {indexer_name} created')  \n",
    "\n",
    "# Run the indexer (if not started right with index creation)\n",
    "#indexer_client.run_indexer(indexer_name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def wait_for_indexer(seconds=15):\n",
    "\n",
    "    while indexer_client.get_indexer_status(indexer_name).last_result == None or indexer_client.get_indexer_status(indexer_name).last_result.status == 'inProgress':\n",
    "        print('Indexer running...')\n",
    "        time.sleep(seconds)\n",
    "        \n",
    "    print(indexer_client.get_indexer_status(indexer_name).last_result.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for indexer to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer running...\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "wait_for_indexer(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
